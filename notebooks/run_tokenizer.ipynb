{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# Example Notebook for Network Visualization"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Change working directory to this root"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import os\n",
                        "os.chdir(\"..\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Imports"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import random\n",
                        "from typing import Any\n",
                        "\n",
                        "import torch\n",
                        "from torch.utils.data import DataLoader, _utils\n",
                        "import matplotlib.pyplot as plt\n",
                        "import torchaudio\n",
                        "import IPython\n",
                        "from torchview import draw_graph\n",
                        "\n",
                        "from utils.config import load_cfg_from_hydra\n",
                        "from utils.containers import MelSpecParameters\n",
                        "from models.mel_spec_converters import SimpleMelSpecConverter"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Load model"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# weights path\n",
                        "import hydra\n",
                        "\n",
                        "from models.modules.base import load_inner_model_state_dict\n",
                        "\n",
                        "weights_path = \"weights/lvl1_vqgan.ckpt\"\n",
                        "# weights_path = \"trained/lvl1_vqvae/model.ckpt\"\n",
                        "\n",
                        "# Infer device\n",
                        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                        "device = \"cpu\"\n",
                        "print(f\"Current device is {device}\")\n",
                        "\n",
                        "# Load network\n",
                        "cfg = load_cfg_from_hydra(config_path=\"../config\", config_name=\"lvl1_vqgan\", overrides=[\"data=lvl1_vqvae\"])\n",
                        "# cfg = load_cfg_from_hydra(\n",
                        "#     config_path=\"../trained/lvl1_vqvae\", config_name=\"config\"\n",
                        "# )\n",
                        "cfg.learning.batch_size = 128\n",
                        "cfg.learning.val_split = 1.0 # Will it let me do that?\n",
                        "model = hydra.utils.instantiate(cfg.module, _convert_=\"partial\").to(device)\n",
                        "model = load_inner_model_state_dict(model, weights_path).eval()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "slice_len = 32\n",
                        "# model_graph = draw_graph(\n",
                        "#     model, input_data = {\"input\": {\"slice\": torch.randn(slice_len, 1, cfg.data.dataset.slice_length)}}, device=device, depth=2\n",
                        "# )\n",
                        "# model_graph.visual_graph"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Load data loader"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "dataset = hydra.utils.instantiate(cfg.data.dataset, _recursive_=False)\n",
                        "dataset_len = len(dataset)\n",
                        "\n",
                        "# Select a random sample\n",
                        "sample_start = random.randint(0, dataset_len - slice_len - 1)\n",
                        "# sample_start = 449\n",
                        "dataset_slice: dict[str, torch.Tensor] = _utils.collate.default_collate([dataset[i] for i in range(sample_start, sample_start + slice_len)]) # type: ignore\n",
                        "concatenated_slice = dataset_slice[\"slice\"]\n",
                        "\n",
                        "print(f\"The concatenated slice dimensions are {concatenated_slice.shape}\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### WAV Player element"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# this is a wrapper that take a filename and publish an html <audio> tag to listen to it\n",
                        "\n",
                        "def wavPlayer(filepath):\n",
                        "    \"\"\" will display html 5 player for compatible browser\n",
                        "\n",
                        "    Parameters :\n",
                        "    ------------\n",
                        "    filepath : relative filepath with respect to the notebook directory ( where the .ipynb are not cwd)\n",
                        "                of the file to play\n",
                        "\n",
                        "    The browser need to know how to play wav through html5.\n",
                        "\n",
                        "    there is no autoplay to prevent file playing when the browser opens\n",
                        "    \"\"\"\n",
                        "    \n",
                        "    src = \"\"\"\n",
                        "    <head>\n",
                        "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
                        "    <title>Simple Test</title>\n",
                        "    </head>\n",
                        "    \n",
                        "    <body>\n",
                        "    <audio controls=\"controls\" style=\"width:600px\" >\n",
                        "        <source src=\"files/%s\" type=\"audio/mp3\" />\n",
                        "        Your browser does not support the audio element.\n",
                        "    </audio>\n",
                        "    </body>\n",
                        "    \"\"\"%(filepath)\n",
                        "    display(HTML(src))"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Initialize Mel-Spectrogram Converters"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "n_mels = 64\n",
                        "mel_spec_params = MelSpecParameters(n_fft=1024, f_min=0, hop_length=256, n_mels=n_mels, power=1.0, pad=0)\n",
                        "mel_spec_converter = SimpleMelSpecConverter(mel_spec_params)\n",
                        "\n",
                        "mel_spec_params_2 = MelSpecParameters(n_fft=2048, f_min=0, hop_length=512, n_mels=128, power=1.0, pad=0)\n",
                        "mel_spec_converter_2 = SimpleMelSpecConverter(mel_spec_params_2)\n",
                        "\n",
                        "mel_spec_params_3 = MelSpecParameters(n_fft=4096, f_min=0, hop_length=1024, n_mels=256, power=1.0, pad=0)\n",
                        "mel_spec_converter_3 = SimpleMelSpecConverter(mel_spec_params_3)\n",
                        "\n",
                        "lin_vector = torch.linspace(\n",
                        "    1.0,\n",
                        "    1.0,\n",
                        "    n_mels,\n",
                        ")\n",
                        "eye_mat = torch.diag(lin_vector).to(device)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Load data point and visualize it"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "image_reshaped = concatenated_slice.flatten().cpu().numpy() # type: ignore\n",
                        "from utils.transform_func import log_normal\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(image_reshaped)\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "# plt.matshow(torch.tanh(eye_mat @ mel_spec_converter.convert(torch.tensor(image_reshaped))).cpu().numpy(),\n",
                        "#             origin='lower', aspect='auto', vmin=0, vmax=1)\n",
                        "plt.matshow(\n",
                        "    log_normal(mel_spec_converter.convert(torch.tensor(image_reshaped))).cpu().numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "plt.matshow(\n",
                        "    log_normal(mel_spec_converter_2.convert(torch.tensor(image_reshaped))).cpu().numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "plt.matshow(\n",
                        "    log_normal(mel_spec_converter_3.convert(torch.tensor(image_reshaped))).cpu().numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "torchaudio.save('sample.mp3', concatenated_slice.flatten().unsqueeze(0).cpu().detach(), 44100, format='mp3') # type: ignore\n",
                        "IPython.display.Audio(filename=\"sample.mp3\") # type: ignore"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Inference\n",
                        "\n",
                        "### Perform inference"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "collated_batch = {\"slice\": concatenated_slice.to(device)}\n",
                        "print(collated_batch[\"slice\"].shape)\n",
                        "with torch.no_grad():\n",
                        "    reconstructed_slice = model(collated_batch)\n",
                        "reconstructed_slice_flattened = reconstructed_slice[\"slice\"].flatten().cpu().detach()\n",
                        "print(torch.nn.functional.smooth_l1_loss(reconstructed_slice[\"slice\"], collated_batch[\"slice\"]))\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(reconstructed_slice_flattened)\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "# plt.matshow(torch.tanh(eye_mat @ mel_spec_converter.convert(torch.tensor(reconstructed_slice[\"slice\"].flatten()))).cpu().numpy(),\n",
                        "#             origin='lower', aspect='auto', vmin=0, vmax=1)\n",
                        "\n",
                        "plt.matshow(\n",
                        "    log_normal(\n",
                        "        mel_spec_converter.convert(torch.tensor(reconstructed_slice[\"slice\"].flatten()))\n",
                        "    )\n",
                        "    .cpu()\n",
                        "    .numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "plt.matshow(\n",
                        "    log_normal(\n",
                        "        mel_spec_converter_2.convert(torch.tensor(reconstructed_slice[\"slice\"].flatten()))\n",
                        "    )\n",
                        "    .cpu()\n",
                        "    .numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "plt.matshow(\n",
                        "    log_normal(\n",
                        "        mel_spec_converter_3.convert(torch.tensor(reconstructed_slice[\"slice\"].flatten()))\n",
                        "    )\n",
                        "    .cpu()\n",
                        "    .numpy(),\n",
                        "    origin=\"lower\",\n",
                        "    aspect=\"auto\",\n",
                        "    vmin=-2,\n",
                        "    vmax=2,\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "torchaudio.save('sample.mp3', reconstructed_slice[\"slice\"].flatten().clip(-1, 1).unsqueeze(0).cpu().detach(), 44100, format='mp3') # type: ignore\n",
                        "IPython.display.Audio(filename=\"sample.mp3\") # type: ignore"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Compare mel specs of a single 1024 length slice"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "single_sample = dataset[sample_start]\n",
                        "collated_sample = {key: torch.stack((value,), dim=0).to(device) for (key, value) in single_sample.items() if key == \"slice\"}\n",
                        "\n",
                        "with torch.no_grad():\n",
                        "    reconstructed_single_sample = model(collated_sample)\n",
                        "    \n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(collated_sample[\"slice\"].flatten().cpu().detach().numpy())\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(reconstructed_single_sample[\"slice\"].flatten().cpu().detach().numpy())\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n",
                        "    \n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.matshow(\n",
                        "    torch.tanh(\n",
                        "        eye_mat @ mel_spec_converter.convert(collated_sample[\"slice\"].flatten())\n",
                        "    ).cpu().numpy(), origin='lower', aspect='auto', vmin=0, vmax=1\n",
                        ")\n",
                        "plt.show()\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.matshow(\n",
                        "    torch.tanh(\n",
                        "        eye_mat @ mel_spec_converter.convert(torch.tensor(reconstructed_single_sample[\"slice\"].flatten()))\n",
                        "    ).cpu().numpy(), origin='lower', aspect='auto', vmin=0, vmax=1\n",
                        ")\n",
                        "plt.show()"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### Try High-Pass Filter"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import torchaudio.functional as AF\n",
                        "\n",
                        "high_pass_response = AF.highpass_biquad(collated_sample[\"slice\"], 44100, 1000)\n",
                        "reconstructed_high_pass_response = AF.highpass_biquad(reconstructed_single_sample[\"slice\"], 44100, 1000)\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(high_pass_response.flatten().cpu().detach().numpy())\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n",
                        "\n",
                        "plt.figure(figsize=(30, 5))\n",
                        "plt.plot(reconstructed_high_pass_response.flatten().cpu().detach().numpy())\n",
                        "\n",
                        "plt.tight_layout()\n",
                        "plt.ylim(-1.2, 1.2)\n",
                        "plt.show()\n"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "infected_pbm",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.12.8"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
