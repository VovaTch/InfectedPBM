_target_: models.models.multi_level_vqvae.decoder.AttentionStftDecoder
input_dim: 32
num_heads: 4
hidden_dim: 128
num_layers: 6
n_fft: 256
hop_length: 64 # Must be equal to the sequence length reduction ration in the encoder
win_length: 256
dropout: 0.1
padding: same